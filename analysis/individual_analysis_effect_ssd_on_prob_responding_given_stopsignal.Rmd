---
title: "Individual level - Effect of stop-signal delay on probability of responding given a stop-signal"
author: "Bram B. Zandbelt"
output:
  html_document:
    toc: yes
    number_sections: yes
    depth: 4
    df_print: paged
    theme: readable
    highlight: pygments
---

# Overview

# Preliminaries
Make `magrittr`'s pipe accessible from notebook.
```{r Provide access to pipe from magrittr }
"%>%" <- magrittr::`%>%`
```

## Identify the project directory
All paths in the code are relative to the project directory.
```{r Define project directory}
project_dir <- rprojroot::find_root(rprojroot::has_file("DESCRIPTION"))
```

## Verify existence of output directories
```{r Define and verify existence of output directories}

derivatives_dir <- file.path(project_dir,"data","derivatives")
figures_dir <- file.path(project_dir,"data","reports","figures")
notebook_name <- "individual_analysis_effect_ssd_on_prob_responding_given_stopsignal"

irmass::verify_output_dirs(base_dirs = list(derivatives_dir, figures_dir),
                   notebook_name = notebook_name)
```

# Read data

Read trial response data from experimental session
```{r Read preprocessed trial data from experimental session}
expt_trial_resp_data <-
  readr::read_csv(file.path(derivatives_dir, "assess_task_performance_criteria", "tidy_expt_trial_resp_data_for_analysis.csv"),
                  col_types = irmass::get_col_types("expt_trial_resp_data")
                  )
```

Inspect data
```{r}
expt_trial_resp_data
```

# Data cleaning and processing

1. get rid of the no-signal and ignore trials
2. make a new variable that encode selective-stopping type

```{r}
data_for_analysis <- 
  expt_trial_resp_data %>%
  dplyr::filter(trial_alt %in% c('SAS', 'SSS'))
  
data_for_plotting <- 
  expt_trial_resp_data %>%
  dplyr::filter(trial_alt %in% c('SAS', 'SSS')) %>%
  dplyr::group_by(subjectIx, trial_alt, t_d) %>%
  dplyr::summarize(p_resp = sum(r_bi) / n()) %>%
  dplyr::ungroup()

data_for_plotting
```

## Summarize data
```{r}

# stop_trial_resp_summary <- 
  
```




## Converting Odds ratios to standardized effect sizes

See here: https://www.meta-analysis.com/downloads/Meta-analysis%20Converting%20among%20effect%20sizes.pdf
```{r}

d = log(OR) * sqrt(3) / pi

```


## Approach
- Do Bayesian logistic regression
- The regression coefficien of ssd reflects the log odds ratio; when ssd is increased by one unit (i.e. 1 second), the probability
- Convert to Cohen's d, according to $d = log(OR) * sqrt(3) / pi$
- Plot prior (Cauchy) and Cohen-d posterior
- Compute Savage Dickey ratio

Question is: is conversion to Cohen's d necessary or not?



# Descriptive statistics

```{r}
(descriptives_p_resp <- 
  data_for_plotting %>%
  tidyr::spread(key = subjectIx, value = p_resp))
```


# Inferential statistics

```{r}

bf_and_params <- 
  tibble::tibble(subjectIx = integer(),
                 model = character(),
                 B = double(),
                 log10B = double(),
                 label = character(),
                 mean_beta0 = double(),
                 mean_beta = double(),
                 mean_zbeta0 = double(),
                 mean_zbeta = double()
                 )

log_reg_params <- 
  tibble::tibble(subjectIx = integer(),
                 params = matrix()
                 )

for (i_subject in unique(data_for_analysis$subjectIx)) {

  # Select data
  df <- 
    data_for_analysis %>%
    dplyr::filter(subjectIx == i_subject) %>%
    dplyr::select(t_d,r_bi) %>%
    dplyr::mutate(r_bi = as.integer(r_bi),
                  t_d = as.numeric(levels(t_d)[t_d])) %>%
    as.data.frame(.)
    
  
  # Bayesian logistic regression & Savage-Dickey density method
  bf_sub <- 
    irmass::test_if_idv(tib = df,
                        y_name = 'r_bi',
                        x_name = 't_d',
                        file_name_root = file.path(figures_dir,notebook_name, sprintf('sub-%.02d',i_subject)),
                        graph_file_type = 'eps')
  
  
  
  bf_and_params <- 
    tibble::add_row(bf_and_params,
                    subjectIx = i_subject,
                    model = bf_sub$bf$model,
                    B = bf_sub$bf$B,
                    log10B = bf_sub$bf$log10B,
                    label = bf_sub$bf$label,
                    mean_beta0 = bf_sub$params['beta0','Mean'],
                    mean_beta = bf_sub$params['beta','Mean'],
                    mean_zbeta0 = bf_sub$params['zbeta0','Mean'],
                    mean_zbeta = bf_sub$params['zbeta','Mean']
                    )
  
  readr::write_csv(as.data.frame(bf_sub$params),
                   path = file.path(derivatives_dir, 
                                    notebook_name, 
                                    sprintf('bayesian_logistic_regression_params_sub-%.02d.csv',i_subject)),
                   col_names = TRUE)
}

```

```{r}
bf_and_params
```

# Data visualization

```{r}

logistic <- function(x){1 / (1 + exp(-x))}

beta0 <- -5.18
beta0L <- -6.23
beta0H <- -4.16

beta <- 14.11
betaL <- 11.22
betaH <- 17.06

my_df <- tibble::tibble(x = seq(0,1,by = 0.005),
                        p_resp_mean = logistic(beta0 + beta * x),
                        p_resp_LL = logistic(beta0L + betaL * x),
                        p_resp_HH = logistic(beta0H + betaH * x),
                        p_resp_LH = logistic(beta0L + betaH * x),
                        p_resp_HL = logistic(beta0H + betaL * x)
                        )

my_df_long <-
  my_df %>%
  tidyr::gather(key = dv, value = p_resp, p_resp_mean:p_resp_HL)

ggplot2::ggplot(my_df_long,
                ggplot2::aes(x = x,
                             y = p_resp,
                             group = dv)
                ) + 
  ggplot2::geom_line()



```

Check out [this code](https://stackoverflow.com/questions/35366499/ggplot2-how-to-combine-histogram-rug-plot-and-logistic-regression-prediction):


# Write data
